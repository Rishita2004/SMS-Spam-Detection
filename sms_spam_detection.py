# -*- coding: utf-8 -*-
"""SMS SPAM DETECTION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H_pcEXyQB-HJTOh4e0QVN9X5T9et2yDK
"""

import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from matplotlib.colors import ListedColormap
from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score
from sklearn import metrics

import pandas as pd

# Try reading the CSV file with different encodings
encodings = ['utf-8', 'latin1', 'ISO-8859-1', 'cp1252']

for encoding in encodings:
    try:
        data = pd.read_csv("/content/spam.csv", encoding=encoding)
        print("File read successfully with encoding:", encoding)
        data.info()
        break  # If successful, exit the loop
    except UnicodeDecodeError:
        print("Could not read the file with encoding:", encoding)

to_drop = ["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"]
data = data.drop(to_drop, axis=1)

data.info()

data.rename(columns={"v1": "Target", "v2": "Text"}, inplace=True)

data.head()

plt.figure(figsize=(12,8))
fg = sns.countplot(x= data["Target"])
fg.set_title("Count Plot of Classes")
fg.set_xlabel("Classes")
fg.set_ylabel("Number of Data points")

data["No_of_Characters"] = data["Text"].apply(len)
data["No_of_Words"]=data.apply(lambda row: nltk.word_tokenize(row["Text"]), axis=1).apply(len)
data["No_of_sentence"]=data.apply(lambda row: nltk.sent_tokenize(row["Text"]), axis=1).apply(len)

data.describe().T

plt.figure(figsize=(12,8))
fg = sns.pairplot(data=data, hue="Target")
plt.show(fg)

data = data[(data["No_of_Characters"]<350)]
data.shape

plt.figure(figsize=(12,8))
fg = sns.pairplot(data=data, hue="Target")
plt.show(fg)

print("The First 5 Texts: ",*data["Text"][:5], sep = "\n")

def Clean(Text):
    sms = re.sub('[^a-zA-Z]', ' ', Text) #Replacing all non-alphabetic characters with a space
    sms = sms.lower() #converting to lowecase
    sms = sms.split()
    sms = ' '.join(sms)
    return sms

data["Clean_Text"] = data["Text"].apply(Clean)
#Lets have a look at a sample of texts after cleaning
print("The First 5 Texts after cleaning:",*data["Clean_Text"][:5], sep = "\n")

data["Tokenize_Text"]=data.apply(lambda row: nltk.word_tokenize(row["Clean_Text"]), axis=1)

print("The First 5 Texts after Tokenizing: ",*data["Tokenize_Text"][:5], sep = "\n")

def remove_stopwords(text):
    stop_words = set(stopwords.words("english"))
    filtered_text = [word for word in text if word not in stop_words]
    return filtered_text

data["Nostopword_Text"] = data["Tokenize_Text"].apply(remove_stopwords)

print("The First 5 Texts after removing the stopwords: ",*data["Nostopword_Text"][:5], sep = "\n")

lemmatizer = WordNetLemmatizer()
def lemmatize_word(text):
    # provide context i.e. part-of-speech
    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in text]
    return lemmas

data["Lemmatized_Text"] = data["Nostopword_Text"].apply(lemmatize_word)
print("The First 5 Texts after lemitization: ",*data["Lemmatized_Text"][:5], sep = "\n")

corpus = []
for i in data["Lemmatized_Text"]:
    msg = ' '.join([row for row in i])
    corpus.append(msg)

tfidf = TfidfVectorizer()
X = tfidf.fit_transform(corpus).toarray()

label_encoder = LabelEncoder()
data["Target"] = label_encoder.fit_transform(data["Target"])

X_train, X_test, y_train, y_test = train_test_split(data["Lemmatized_Text"], data["Target"], test_size=0.2, random_state=42)

corpus_train = []
corpus_test = []

for i in X_train:
    msg = ' '.join([row for row in i])
    corpus_train.append(msg)

for i in X_test:
    msg = ' '.join([row for row in i])
    corpus_test.append(msg)

tfidf = TfidfVectorizer()
X_train_transformed = tfidf.fit_transform(corpus_train).toarray()
X_test_transformed = tfidf.transform(corpus_test).toarray()

label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

classifiers = [MultinomialNB(),
               RandomForestClassifier(),
               KNeighborsClassifier(),
               SVC()]
for cls in classifiers:
    cls.fit(X_train_transformed, y_train)

precision = []
recall = []
f1_score = []
trainset_accuracy = []
testset_accuracy = []

for cls in classifiers:
    pred_train = cls.predict(X_train_transformed)
    pred_test = cls.predict(X_test_transformed)
    prec = metrics.precision_score(y_test, pred_test)
    recal = metrics.recall_score(y_test, pred_test)
    f1_s = metrics.f1_score(y_test, pred_test)
    train_accuracy = cls.score(X_train_transformed, y_train)
    test_accuracy = cls.score(X_test_transformed, y_test)

    # Appending scores
    precision.append(prec)
    recall.append(recal)
    f1_score.append(f1_s)
    trainset_accuracy.append(train_accuracy)
    testset_accuracy.append(test_accuracy)

data = {'Precision': precision,
        'Recall': recall,
        'F1score': f1_score,
        'Accuracy on Testset': testset_accuracy,
        'Accuracy on Trainset': trainset_accuracy}

Results = pd.DataFrame(data, index=["NaiveBayes", "RandomForest", "KNeighbours", "SVC"])

Results